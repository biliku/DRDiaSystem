# -*- coding: utf-8 -*-
import os
os.environ['ALBUMENTATIONS_DISABLE_VERSION_CHECK'] = '1'

import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import albumentations as A
from albumentations.pytorch import ToTensorV2
import segmentation_models_pytorch as smp
from tqdm import tqdm
import warnings
from collections import OrderedDict

# =================================================================================
# 1. é…ç½®å‚æ•° - æ¨ç†é…ç½®
# =================================================================================
# æ¨¡å‹é…ç½®ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
IMG_SIZE = 512
BATCH_SIZE = 4  # æ¨ç†æ—¶å¯ä»¥é€‚å½“å‡å°æ‰¹æ¬¡å¤§å°
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ç—…ç¶æ˜ å°„ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
LESION_MAP = OrderedDict([
    ('SE', ('4. Soft Exudates', 4)),
    ('MA', ('1. Microaneurysms', 1)),
    ('HE', ('2. Haemorrhages', 2)),
    ('EX', ('3. Hard Exudates', 3))
])
NUM_CLASSES = len(LESION_MAP) + 1

# è·¯å¾„é…ç½®
MODEL_PATH = r"django\DRDiaSys\diagnosis\best_lesion_segmentation_model_v4.pth"
NEW_DATASET_DIR = r"F:\DRDiaSys\django\DRDiaSys\datasets\dataset\aptos2019_preprocessed\test_images_processed"  # æ–°æ•°æ®é›†è·¯å¾„
RESULT_DIR = "prediction_results"  # ç»“æœä¿å­˜ç›®å½•

# =================================================================================
# 2. æ•°æ®é¢„å¤„ç† - ä»…ç”¨äºæ¨ç†
# =================================================================================
def get_inference_transforms():
    """æ¨ç†æ—¶çš„æ•°æ®é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶éªŒè¯é›†ä¿æŒä¸€è‡´ï¼‰"""
    return A.Compose([
        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=cv2.INTER_LINEAR),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2(),
    ])

# =================================================================================
# 3. æ¨ç†ä¸“ç”¨æ•°æ®é›†ç±»
# =================================================================================
class InferenceDataset(Dataset):
    """ä¸“é—¨ç”¨äºæ¨ç†çš„æ•°æ®é›†ç±»ï¼Œåªéœ€è¦å›¾åƒï¼Œä¸éœ€è¦æ ‡æ³¨"""
    
    def __init__(self, image_dir, transform=None):
        self.image_dir = image_dir
        self.transform = transform
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        self.image_names = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']:
            self.image_names.extend([f for f in os.listdir(image_dir) 
                                   if f.lower().endswith(ext.replace('*', ''))])
        
        self.image_names = sorted(self.image_names)
        print(f"æ‰¾åˆ° {len(self.image_names)} å¼ å›¾åƒç”¨äºæ¨ç†")
        
    def __len__(self):
        return len(self.image_names)
    
    def __getitem__(self, idx):
        img_name = self.image_names[idx]
        img_path = os.path.join(self.image_dir, img_name)
        
        # è¯»å–å›¾åƒ
        image = cv2.imread(img_path)
        if image is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")
            
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        original_size = image.shape[:2]  # ä¿å­˜åŸå§‹å°ºå¯¸ (H, W)
        
        # åº”ç”¨é¢„å¤„ç†
        if self.transform:
            transformed = self.transform(image=image)
            image = transformed['image']
        
        return image, img_name, original_size

def custom_collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°"""
    images = []
    img_names = []
    original_sizes = []
    
    for item in batch:
        images.append(item[0])
        img_names.append(item[1])
        original_sizes.append(item[2])
    
    images = torch.stack(images, dim=0)
    return images, img_names, original_sizes

# =================================================================================
# 4. æ¨ç†å’Œç»“æœä¿å­˜å‡½æ•°
# =================================================================================
def load_model(model_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    model = smp.Unet("resnet34", encoder_weights="imagenet", 
                     in_channels=3, classes=NUM_CLASSES)
    
    # åŠ è½½æƒé‡
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {device}")
    return model

def predict_images(model, dataloader, device, result_dir):
    """å¯¹å›¾åƒè¿›è¡Œé¢„æµ‹å¹¶ä¿å­˜ç»“æœ"""
    
    if not os.path.exists(result_dir):
        os.makedirs(result_dir)
    
    # é¢œè‰²æ˜ å°„
    color_map = {
        0: (0,0,0),      # èƒŒæ™¯ - é»‘è‰²
        1: (0,0,255),    # MA (Microaneurysms) - å¾®åŠ¨è„‰ç˜¤ - è“è‰²
        2: (0,255,0),    # HE (Haemorrhages) - å‡ºè¡€ - ç»¿è‰²
        3: (255,0,0),    # EX (Hard Exudates) - ç¡¬æ€§æ¸—å‡ºç‰© - çº¢è‰²
        4: (255,255,0)   # SE (Soft Exudates) - è½¯æ€§æ¸—å‡ºç‰© - é»„è‰²
    }
    
    lesion_names = {
        0: "èƒŒæ™¯",
        1: "å¾®åŠ¨è„‰ç˜¤(MA)",
        2: "å‡ºè¡€(HE)", 
        3: "ç¡¬æ¸—å‡º(EX)",
        4: "è½¯æ¸—å‡º(SE)"
    }
    
    print(f"ğŸ¨ å¼€å§‹é¢„æµ‹ï¼Œç»“æœå°†ä¿å­˜åˆ°: {result_dir}")
    
    with torch.no_grad():
        for batch_idx, (images, img_names, original_sizes) in enumerate(tqdm(dataloader, desc="é¢„æµ‹ä¸­")):
            images = images.to(device)
            
            # æ¨¡å‹æ¨ç†
            outputs = model(images)
            pred_masks = torch.argmax(outputs, dim=1)
            
            # å¤„ç†æ¯å¼ å›¾åƒ
            for i in range(images.size(0)):
                img_name = img_names[i]
                pred_mask = pred_masks[i].cpu().numpy()
                original_size = original_sizes[i]  # (H, W)
                
                # åæ ‡å‡†åŒ–åŸå§‹å›¾åƒ
                img_tensor = images[i].cpu()
                mean = torch.tensor([0.485, 0.456, 0.406])
                std = torch.tensor([0.229, 0.224, 0.225])
                img_np = img_tensor.permute(1, 2, 0).numpy()
                img_np = ((img_np * std.numpy() + mean.numpy()) * 255).astype(np.uint8)
                
                # è°ƒæ•´å›åŸå§‹å°ºå¯¸ (W, H) - OpenCVæ ¼å¼
                original_width = original_size[1]
                original_height = original_size[0]
                
                img_np = cv2.resize(img_np, (original_width, original_height))
                pred_mask = cv2.resize(pred_mask.astype(np.uint8), 
                                     (original_width, original_height), 
                                     interpolation=cv2.INTER_NEAREST)
                
                # åˆ›å»ºå½©è‰²åˆ†å‰²ç»“æœ
                colored_mask = np.zeros_like(img_np)
                for class_id, color in color_map.items():
                    colored_mask[pred_mask == class_id] = color
                
                # åˆ›å»ºå åŠ å›¾åƒ
                overlay = cv2.addWeighted(img_np, 0.7, colored_mask, 0.3, 0)
                
                # æ‹¼æ¥ç»“æœå›¾åƒï¼šåŸå›¾ | åˆ†å‰²ç»“æœ | å åŠ å›¾åƒ
                result_img = np.hstack([img_np, colored_mask, overlay])
                
                # ä¿å­˜ç»“æœ
                base_name = os.path.splitext(img_name)[0]
                save_path = os.path.join(result_dir, f"{base_name}_prediction.jpg")
                cv2.imwrite(save_path, cv2.cvtColor(result_img, cv2.COLOR_RGB2BGR))
                
                # ç»Ÿè®¡å„ç±»åˆ«åƒç´ æ•°é‡
                unique_classes, counts = np.unique(pred_mask, return_counts=True)
                total_pixels = pred_mask.size
                
                print(f"\nğŸ“Š {img_name} é¢„æµ‹ç»Ÿè®¡:")
                for class_id, count in zip(unique_classes, counts):
                    percentage = (count / total_pixels) * 100
                    print(f"   {lesion_names[class_id]}: {count:,} åƒç´  ({percentage:.3f}%)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨çœ¼åº•å›¾åƒç—…ç¶åˆ†å‰²æ¨ç†")
    print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(NEW_DATASET_DIR):
        print(f"âŒ é”™è¯¯: æ–°æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {NEW_DATASET_DIR}")
        return
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
        return
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    dataset = InferenceDataset(NEW_DATASET_DIR, transform=get_inference_transforms())
    if len(dataset) == 0:
        print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶")
        return
    
    # ä½¿ç”¨è‡ªå®šä¹‰çš„æ‰¹å¤„ç†å‡½æ•°
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, 
                          num_workers=0, pin_memory=True, collate_fn=custom_collate_fn)
    
    # åŠ è½½æ¨¡å‹
    model = load_model(MODEL_PATH, DEVICE)
    
    # è¿›è¡Œé¢„æµ‹
    predict_images(model, dataloader, DEVICE, RESULT_DIR)
    
    print(f"\nğŸ‰ é¢„æµ‹å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {RESULT_DIR}")
    print(f"ğŸ’¡ å›¾åƒæ ¼å¼: [åŸå›¾ | åˆ†å‰²ç»“æœ | å åŠ å›¾åƒ]")

if __name__ == '__main__':
    warnings.filterwarnings("ignore")
    main()
